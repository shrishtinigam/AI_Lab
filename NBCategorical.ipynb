{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CupYDUuT_1Cy",
        "outputId": "3b50512c-21c6-40ff-b35d-dc95688702e0"
      },
      "id": "CupYDUuT_1Cy",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/gdrive/My Drive/Colab_csvs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q-xUMUP__s0",
        "outputId": "95233e0f-125e-4b27-bb7f-44a47520eaef"
      },
      "id": "8q-xUMUP__s0",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Colab_csvs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8b4585d4",
      "metadata": {
        "id": "8b4585d4"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import CategoricalNB\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "86480076",
      "metadata": {
        "id": "86480076"
      },
      "outputs": [],
      "source": [
        "ds = pd.read_csv(\"Nephritis.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d07b2d9e",
      "metadata": {
        "scrolled": false,
        "id": "d07b2d9e",
        "outputId": "90106bc7-a3fe-4659-8091-95fb848218d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 rows : \n",
            "\n",
            "   Temperature  Nausea_no  Nausea_yes  Lumbar pain_no  Lumbar pain_yes  \\\n",
            "0         35.5          1           0               0                1   \n",
            "1         35.9          1           0               1                0   \n",
            "2         35.9          1           0               0                1   \n",
            "3         36.0          1           0               1                0   \n",
            "4         36.0          1           0               0                1   \n",
            "\n",
            "   Continous_need_no  Continous_need_yes  Micturition pains_no  \\\n",
            "0                  1                   0                     1   \n",
            "1                  0                   1                     0   \n",
            "2                  1                   0                     1   \n",
            "3                  0                   1                     0   \n",
            "4                  1                   0                     1   \n",
            "\n",
            "   Micturition pains_yes  Burning_no  Burning_yes  \n",
            "0                      0           1            0  \n",
            "1                      1           0            1  \n",
            "2                      0           1            0  \n",
            "3                      1           0            1  \n",
            "4                      0           1            0  \n"
          ]
        }
      ],
      "source": [
        "y = ds.Nephritis.values\n",
        "new_data = pd.get_dummies(ds.drop(['Nephritis'],axis = 1)) #encoding\n",
        "print(f\"First 5 rows : \\n\\n{new_data.head(5)}\")\n",
        "x = (new_data-np.min(new_data))/(np.max(new_data)-np.min(new_data)).values\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(CategoricalNB())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rxyqa0tFAY3P",
        "outputId": "cc4eae58-04c3-4165-8a27-76ce0ac50d53"
      },
      "id": "Rxyqa0tFAY3P",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on CategoricalNB in module sklearn.naive_bayes object:\n",
            "\n",
            "class CategoricalNB(_BaseDiscreteNB)\n",
            " |  CategoricalNB(*, alpha=1.0, fit_prior=True, class_prior=None, min_categories=None)\n",
            " |  \n",
            " |  Naive Bayes classifier for categorical features.\n",
            " |  \n",
            " |  The categorical Naive Bayes classifier is suitable for classification with\n",
            " |  discrete features that are categorically distributed. The categories of\n",
            " |  each feature are drawn from a categorical distribution.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <categorical_naive_bayes>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  alpha : float, default=1.0\n",
            " |      Additive (Laplace/Lidstone) smoothing parameter\n",
            " |      (0 for no smoothing).\n",
            " |  \n",
            " |  fit_prior : bool, default=True\n",
            " |      Whether to learn class prior probabilities or not.\n",
            " |      If false, a uniform prior will be used.\n",
            " |  \n",
            " |  class_prior : array-like of shape (n_classes,), default=None\n",
            " |      Prior probabilities of the classes. If specified the priors are not\n",
            " |      adjusted according to the data.\n",
            " |  \n",
            " |  min_categories : int or array-like of shape (n_features,), default=None\n",
            " |      Minimum number of categories per feature.\n",
            " |  \n",
            " |      - integer: Sets the minimum number of categories per feature to\n",
            " |        `n_categories` for each features.\n",
            " |      - array-like: shape (n_features,) where `n_categories[i]` holds the\n",
            " |        minimum number of categories for the ith column of the input.\n",
            " |      - None (default): Determines the number of categories automatically\n",
            " |        from the training data.\n",
            " |  \n",
            " |      .. versionadded:: 0.24\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  category_count_ : list of arrays of shape (n_features,)\n",
            " |      Holds arrays of shape (n_classes, n_categories of respective feature)\n",
            " |      for each feature. Each array provides the number of samples\n",
            " |      encountered for each class and category of the specific feature.\n",
            " |  \n",
            " |  class_count_ : ndarray of shape (n_classes,)\n",
            " |      Number of samples encountered for each class during fitting. This\n",
            " |      value is weighted by the sample weight when provided.\n",
            " |  \n",
            " |  class_log_prior_ : ndarray of shape (n_classes,)\n",
            " |      Smoothed empirical log probability for each class.\n",
            " |  \n",
            " |  classes_ : ndarray of shape (n_classes,)\n",
            " |      Class labels known to the classifier\n",
            " |  \n",
            " |  feature_log_prob_ : list of arrays of shape (n_features,)\n",
            " |      Holds arrays of shape (n_classes, n_categories of respective feature)\n",
            " |      for each feature. Each array provides the empirical log probability\n",
            " |      of categories given the respective feature and class, ``P(x_i|y)``.\n",
            " |  \n",
            " |  n_features_ : int\n",
            " |      Number of features of each sample.\n",
            " |  \n",
            " |      .. deprecated:: 1.0\n",
            " |          Attribute `n_features_` was deprecated in version 1.0 and will be\n",
            " |          removed in 1.2. Use `n_features_in_` instead.\n",
            " |  \n",
            " |  n_features_in_ : int\n",
            " |      Number of features seen during :term:`fit`.\n",
            " |  \n",
            " |      .. versionadded:: 0.24\n",
            " |  \n",
            " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
            " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
            " |      has feature names that are all strings.\n",
            " |  \n",
            " |      .. versionadded:: 1.0\n",
            " |  \n",
            " |  n_categories_ : ndarray of shape (n_features,), dtype=np.int64\n",
            " |      Number of categories for each feature. This value is\n",
            " |      inferred from the data or set by the minimum number of categories.\n",
            " |  \n",
            " |      .. versionadded:: 0.24\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\n",
            " |  ComplementNB : Complement Naive Bayes classifier.\n",
            " |  GaussianNB : Gaussian Naive Bayes.\n",
            " |  MultinomialNB : Naive Bayes classifier for multinomial models.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> import numpy as np\n",
            " |  >>> rng = np.random.RandomState(1)\n",
            " |  >>> X = rng.randint(5, size=(6, 100))\n",
            " |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
            " |  >>> from sklearn.naive_bayes import CategoricalNB\n",
            " |  >>> clf = CategoricalNB()\n",
            " |  >>> clf.fit(X, y)\n",
            " |  CategoricalNB()\n",
            " |  >>> print(clf.predict(X[2:3]))\n",
            " |  [3]\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      CategoricalNB\n",
            " |      _BaseDiscreteNB\n",
            " |      _BaseNB\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *, alpha=1.0, fit_prior=True, class_prior=None, min_categories=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None)\n",
            " |      Fit Naive Bayes classifier according to X, y.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          Training vectors, where `n_samples` is the number of samples and\n",
            " |          `n_features` is the number of features. Here, each feature of X is\n",
            " |          assumed to be from a different categorical distribution.\n",
            " |          It is further assumed that all categories of each feature are\n",
            " |          represented by the numbers 0, ..., n - 1, where n refers to the\n",
            " |          total number of categories for the given feature. This can, for\n",
            " |          instance, be achieved with the help of OrdinalEncoder.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,)\n",
            " |          Target values.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Weights applied to individual samples (1. for unweighted).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Returns the instance itself.\n",
            " |  \n",
            " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
            " |      Incremental fit on a batch of samples.\n",
            " |      \n",
            " |      This method is expected to be called several times consecutively\n",
            " |      on different chunks of a dataset so as to implement out-of-core\n",
            " |      or online learning.\n",
            " |      \n",
            " |      This is especially useful when the whole dataset is too big to fit in\n",
            " |      memory at once.\n",
            " |      \n",
            " |      This method has some performance overhead hence it is better to call\n",
            " |      partial_fit on chunks of data that are as large as possible\n",
            " |      (as long as fitting in the memory budget) to hide the overhead.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          Training vectors, where `n_samples` is the number of samples and\n",
            " |          `n_features` is the number of features. Here, each feature of X is\n",
            " |          assumed to be from a different categorical distribution.\n",
            " |          It is further assumed that all categories of each feature are\n",
            " |          represented by the numbers 0, ..., n - 1, where n refers to the\n",
            " |          total number of categories for the given feature. This can, for\n",
            " |          instance, be achieved with the help of OrdinalEncoder.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,)\n",
            " |          Target values.\n",
            " |      \n",
            " |      classes : array-like of shape (n_classes,), default=None\n",
            " |          List of all the classes that can possibly appear in the y vector.\n",
            " |      \n",
            " |          Must be provided at the first call to partial_fit, can be omitted\n",
            " |          in subsequent calls.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Weights applied to individual samples (1. for unweighted).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Returns the instance itself.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from _BaseDiscreteNB:\n",
            " |  \n",
            " |  coef_\n",
            " |      DEPRECATED: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
            " |  \n",
            " |  intercept_\n",
            " |      DEPRECATED: Attribute `intercept_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
            " |  \n",
            " |  n_features_\n",
            " |      DEPRECATED: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from _BaseNB:\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Perform classification on an array of test vectors X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          The input samples.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : ndarray of shape (n_samples,)\n",
            " |          Predicted target values for X.\n",
            " |  \n",
            " |  predict_log_proba(self, X)\n",
            " |      Return log-probability estimates for the test vector X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          The input samples.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : array-like of shape (n_samples, n_classes)\n",
            " |          Returns the log-probability of the samples for each class in\n",
            " |          the model. The columns correspond to the classes in sorted\n",
            " |          order, as they appear in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  predict_proba(self, X)\n",
            " |      Return probability estimates for the test vector X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          The input samples.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : array-like of shape (n_samples, n_classes)\n",
            " |          Returns the probability of the samples for each class in\n",
            " |          the model. The columns correspond to the classes in sorted\n",
            " |          order, as they appear in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for `X`.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a8612e9",
      "metadata": {
        "scrolled": false,
        "id": "5a8612e9",
        "outputId": "28b2b44c-ebaa-4459-bb89-a60cd01b29fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Trained model parameters:\n",
            "\n",
            "The class category count is:  [array([[61.,  1.],\n",
            "       [32.,  2.]]), array([[ 0., 62.],\n",
            "       [18., 16.]]), array([[62.,  0.],\n",
            "       [16., 18.]]), array([[19., 43.],\n",
            "       [34.,  0.]]), array([[43., 19.],\n",
            "       [ 0., 34.]]), array([[33., 29.],\n",
            "       [30.,  4.]]), array([[29., 33.],\n",
            "       [ 4., 30.]]), array([[25., 37.],\n",
            "       [18., 16.]]), array([[37., 25.],\n",
            "       [16., 18.]]), array([[19., 43.],\n",
            "       [23., 11.]]), array([[43., 19.],\n",
            "       [11., 23.]])] \n",
            "\n",
            "The class count is:  [62. 34.] \n",
            "\n",
            "The class log prior is:  [-0.43721381 -1.03798767] \n",
            "\n",
            "The classes are:  ['no' 'yes'] \n",
            "\n",
            "The feature log probability is:  [array([[-0.0317487 , -3.4657359 ],\n",
            "       [-0.08701138, -2.48490665]]), array([[-4.15888308, -0.01574836],\n",
            "       [-0.63907996, -0.75030559]]), array([[-0.01574836, -4.15888308],\n",
            "       [-0.75030559, -0.63907996]]), array([[-1.16315081, -0.37469345],\n",
            "       [-0.02817088, -3.58351894]]), array([[-0.37469345, -1.16315081],\n",
            "       [-3.58351894, -0.02817088]]), array([[-0.63252256, -0.7576857 ],\n",
            "       [-0.14953173, -1.97408103]]), array([[-0.7576857 , -0.63252256],\n",
            "       [-1.97408103, -0.14953173]]), array([[-0.90078655, -0.52129692],\n",
            "       [-0.63907996, -0.75030559]]), array([[-0.52129692, -0.90078655],\n",
            "       [-0.75030559, -0.63907996]]), array([[-1.16315081, -0.37469345],\n",
            "       [-0.40546511, -1.09861229]]), array([[-0.37469345, -1.16315081],\n",
            "       [-1.09861229, -0.40546511]])] \n",
            "\n",
            "The number of features are:  11 \n",
            "\n",
            "The categories are:  [2 2 2 2 2 2 2 2 2 2 2] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = CategoricalNB()\n",
        "model.fit(x_train, y_train)\n",
        "print(\"\\n\\n\\nTrained model parameters:\")\n",
        "print(\"\\nThe class category count is: \",model.category_count_,\"\\n\")\n",
        "print(\"The class count is: \",model.class_count_,\"\\n\") \n",
        "print(\"The class log prior is: \",model.class_log_prior_,\"\\n\") \n",
        "print(\"The classes are: \",model.classes_,\"\\n\") \n",
        "print(\"The feature log probability is: \",model.feature_log_prob_,\"\\n\") \n",
        "print(\"The number of features are: \",model.n_features_,\"\\n\") \n",
        "print(\"The categories are: \",model.n_categories_,\"\\n\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0848b5b7",
      "metadata": {
        "id": "0848b5b7",
        "outputId": "0e1bf840-071e-4817-ac87-bc324e7bdfb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CategoricalNB()"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#test data\n",
        "nb_model = CategoricalNB()\n",
        "nb_model.fit(x_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4370cd9",
      "metadata": {
        "id": "c4370cd9",
        "outputId": "4255a4e4-ba23-4c82-a665-4826ca6ea65a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 8  0]\n",
            " [ 0 16]]\n"
          ]
        }
      ],
      "source": [
        "#confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_predicted = model.predict(x_test)\n",
        "cmatrix = confusion_matrix(y_test,y_predicted) \n",
        "print(cmatrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae622f1f",
      "metadata": {
        "id": "ae622f1f",
        "outputId": "da540c8e-dc63-492a-8471-a161f5736491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Accuracy of the Naive Bayes classifier is : 0.9583333333333334\n",
            "Hence, this concludes that 95.83333333333334% of samples are classified correctly\n"
          ]
        }
      ],
      "source": [
        "# Report accuracy.\n",
        "accuracy_nb = nb_model.score(x_test,y_test)\n",
        "print(f\"The Accuracy of the Naive Bayes classifier is : {accuracy_nb}\")\n",
        "print(f\"Hence, this concludes that {accuracy_nb*100}% of samples are classified correctly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b748ca2b",
      "metadata": {
        "id": "b748ca2b"
      },
      "source": [
        "### Changing parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1cd02a3",
      "metadata": {
        "id": "a1cd02a3"
      },
      "outputs": [],
      "source": [
        "def CategoricalNBClass(a,mc):\n",
        "    nb_model = CategoricalNB(alpha = a, min_categories = mc)\n",
        "    nb_model.fit(x_test,y_test)\n",
        "    \n",
        "    # Confusion matrix\n",
        "    y_predicted = model.predict(x_test)\n",
        "    cmatrix = confusion_matrix(y_test,y_predicted)\n",
        "    print(f\"Confusion Matrix is : \\n{cmatrix}\")\n",
        "    \n",
        "    #Accuracy\n",
        "    accuracy_nb = nb_model.score(x_test,y_test)\n",
        "    print(f\"The Accuracy of the Naive Bayes classifier is : {accuracy_nb}\")\n",
        "    print(f\"Hence, this concludes that {accuracy_nb*100}% of samples are classified correctly\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3264ff05",
      "metadata": {
        "id": "3264ff05",
        "outputId": "b6212127-5f67-4c3b-8e91-2b0d5c7d7639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix is : \n",
            "[[ 8  0]\n",
            " [ 0 16]]\n",
            "The Accuracy of the Naive Bayes classifier is : 0.9583333333333334\n",
            "Hence, this concludes that 95.83333333333334% of samples are classified correctly\n"
          ]
        }
      ],
      "source": [
        "CategoricalNBClass(0.25,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3841df71",
      "metadata": {
        "id": "3841df71",
        "outputId": "dfcd58af-d86f-4b07-c5a4-f767c904fad2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix is : \n",
            "[[ 8  0]\n",
            " [ 0 16]]\n",
            "The Accuracy of the Naive Bayes classifier is : 0.9583333333333334\n",
            "Hence, this concludes that 95.83333333333334% of samples are classified correctly\n",
            "\n",
            "\n",
            "\n",
            "Confusion Matrix is : \n",
            "[[ 8  0]\n",
            " [ 0 16]]\n",
            "The Accuracy of the Naive Bayes classifier is : 0.9583333333333334\n",
            "Hence, this concludes that 95.83333333333334% of samples are classified correctly\n",
            "\n",
            "\n",
            "\n",
            "Confusion Matrix is : \n",
            "[[ 8  0]\n",
            " [ 0 16]]\n",
            "The Accuracy of the Naive Bayes classifier is : 0.9166666666666666\n",
            "Hence, this concludes that 91.66666666666666% of samples are classified correctly\n",
            "\n",
            "\n",
            "\n",
            "Confusion Matrix is : \n",
            "[[ 8  0]\n",
            " [ 0 16]]\n",
            "The Accuracy of the Naive Bayes classifier is : 0.9166666666666666\n",
            "Hence, this concludes that 91.66666666666666% of samples are classified correctly\n"
          ]
        }
      ],
      "source": [
        "CategoricalNBClass(0.5,1)\n",
        "print(\"\\n\\n\")\n",
        "CategoricalNBClass(0.5,200)\n",
        "print(\"\\n\\n\")\n",
        "CategoricalNBClass(0.5,1000)\n",
        "print(\"\\n\\n\")\n",
        "CategoricalNBClass(0.5,2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba1a2781",
      "metadata": {
        "id": "ba1a2781",
        "outputId": "58922eaa-9c30-4f83-c503-51b53c8e22d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix is : \n",
            "[[ 8  0]\n",
            " [ 0 16]]\n",
            "The Accuracy of the Naive Bayes classifier is : 0.9583333333333334\n",
            "Hence, this concludes that 95.83333333333334% of samples are classified correctly\n"
          ]
        }
      ],
      "source": [
        "CategoricalNBClass(0.75,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "becc8023",
      "metadata": {
        "id": "becc8023",
        "outputId": "8fa2d18c-b4c2-4eff-9280-ce1e44175eff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix is : \n",
            "[[ 8  0]\n",
            " [ 0 16]]\n",
            "The Accuracy of the Naive Bayes classifier is : 0.9583333333333334\n",
            "Hence, this concludes that 95.83333333333334% of samples are classified correctly\n"
          ]
        }
      ],
      "source": [
        "CategoricalNBClass(1,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e64db3c",
      "metadata": {
        "scrolled": false,
        "id": "7e64db3c",
        "outputId": "e83e224b-2eb1-4ea3-fdab-087fd5ec146f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on CategoricalNB in module sklearn.naive_bayes object:\n",
            "\n",
            "class CategoricalNB(_BaseDiscreteNB)\n",
            " |  CategoricalNB(*, alpha=1.0, fit_prior=True, class_prior=None, min_categories=None)\n",
            " |  \n",
            " |  Naive Bayes classifier for categorical features\n",
            " |  \n",
            " |  The categorical Naive Bayes classifier is suitable for classification with\n",
            " |  discrete features that are categorically distributed. The categories of\n",
            " |  each feature are drawn from a categorical distribution.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <categorical_naive_bayes>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  alpha : float, default=1.0\n",
            " |      Additive (Laplace/Lidstone) smoothing parameter\n",
            " |      (0 for no smoothing).\n",
            " |  \n",
            " |  fit_prior : bool, default=True\n",
            " |      Whether to learn class prior probabilities or not.\n",
            " |      If false, a uniform prior will be used.\n",
            " |  \n",
            " |  class_prior : array-like of shape (n_classes,), default=None\n",
            " |      Prior probabilities of the classes. If specified the priors are not\n",
            " |      adjusted according to the data.\n",
            " |  \n",
            " |  min_categories : int or array-like of shape (n_features,), default=None\n",
            " |      Minimum number of categories per feature.\n",
            " |  \n",
            " |      - integer: Sets the minimum number of categories per feature to\n",
            " |        `n_categories` for each features.\n",
            " |      - array-like: shape (n_features,) where `n_categories[i]` holds the\n",
            " |        minimum number of categories for the ith column of the input.\n",
            " |      - None (default): Determines the number of categories automatically\n",
            " |        from the training data.\n",
            " |  \n",
            " |      .. versionadded:: 0.24\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  category_count_ : list of arrays of shape (n_features,)\n",
            " |      Holds arrays of shape (n_classes, n_categories of respective feature)\n",
            " |      for each feature. Each array provides the number of samples\n",
            " |      encountered for each class and category of the specific feature.\n",
            " |  \n",
            " |  class_count_ : ndarray of shape (n_classes,)\n",
            " |      Number of samples encountered for each class during fitting. This\n",
            " |      value is weighted by the sample weight when provided.\n",
            " |  \n",
            " |  class_log_prior_ : ndarray of shape (n_classes,)\n",
            " |      Smoothed empirical log probability for each class.\n",
            " |  \n",
            " |  classes_ : ndarray of shape (n_classes,)\n",
            " |      Class labels known to the classifier\n",
            " |  \n",
            " |  feature_log_prob_ : list of arrays of shape (n_features,)\n",
            " |      Holds arrays of shape (n_classes, n_categories of respective feature)\n",
            " |      for each feature. Each array provides the empirical log probability\n",
            " |      of categories given the respective feature and class, ``P(x_i|y)``.\n",
            " |  \n",
            " |  n_features_ : int\n",
            " |      Number of features of each sample.\n",
            " |  \n",
            " |  n_categories_ : ndarray of shape (n_features,), dtype=np.int64\n",
            " |      Number of categories for each feature. This value is\n",
            " |      inferred from the data or set by the minimum number of categories.\n",
            " |  \n",
            " |      .. versionadded:: 0.24\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> import numpy as np\n",
            " |  >>> rng = np.random.RandomState(1)\n",
            " |  >>> X = rng.randint(5, size=(6, 100))\n",
            " |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
            " |  >>> from sklearn.naive_bayes import CategoricalNB\n",
            " |  >>> clf = CategoricalNB()\n",
            " |  >>> clf.fit(X, y)\n",
            " |  CategoricalNB()\n",
            " |  >>> print(clf.predict(X[2:3]))\n",
            " |  [3]\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      CategoricalNB\n",
            " |      _BaseDiscreteNB\n",
            " |      _BaseNB\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *, alpha=1.0, fit_prior=True, class_prior=None, min_categories=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None)\n",
            " |      Fit Naive Bayes classifier according to X, y\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          Training vectors, where n_samples is the number of samples and\n",
            " |          n_features is the number of features. Here, each feature of X is\n",
            " |          assumed to be from a different categorical distribution.\n",
            " |          It is further assumed that all categories of each feature are\n",
            " |          represented by the numbers 0, ..., n - 1, where n refers to the\n",
            " |          total number of categories for the given feature. This can, for\n",
            " |          instance, be achieved with the help of OrdinalEncoder.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,)\n",
            " |          Target values.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples), default=None\n",
            " |          Weights applied to individual samples (1. for unweighted).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |  \n",
            " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
            " |      Incremental fit on a batch of samples.\n",
            " |      \n",
            " |      This method is expected to be called several times consecutively\n",
            " |      on different chunks of a dataset so as to implement out-of-core\n",
            " |      or online learning.\n",
            " |      \n",
            " |      This is especially useful when the whole dataset is too big to fit in\n",
            " |      memory at once.\n",
            " |      \n",
            " |      This method has some performance overhead hence it is better to call\n",
            " |      partial_fit on chunks of data that are as large as possible\n",
            " |      (as long as fitting in the memory budget) to hide the overhead.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          Training vectors, where n_samples is the number of samples and\n",
            " |          n_features is the number of features. Here, each feature of X is\n",
            " |          assumed to be from a different categorical distribution.\n",
            " |          It is further assumed that all categories of each feature are\n",
            " |          represented by the numbers 0, ..., n - 1, where n refers to the\n",
            " |          total number of categories for the given feature. This can, for\n",
            " |          instance, be achieved with the help of OrdinalEncoder.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples)\n",
            " |          Target values.\n",
            " |      \n",
            " |      classes : array-like of shape (n_classes), default=None\n",
            " |          List of all the classes that can possibly appear in the y vector.\n",
            " |      \n",
            " |          Must be provided at the first call to partial_fit, can be omitted\n",
            " |          in subsequent calls.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples), default=None\n",
            " |          Weights applied to individual samples (1. for unweighted).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from _BaseDiscreteNB:\n",
            " |  \n",
            " |  coef_\n",
            " |  \n",
            " |  intercept_\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from _BaseNB:\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Perform classification on an array of test vectors X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : ndarray of shape (n_samples,)\n",
            " |          Predicted target values for X\n",
            " |  \n",
            " |  predict_log_proba(self, X)\n",
            " |      Return log-probability estimates for the test vector X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : array-like of shape (n_samples, n_classes)\n",
            " |          Returns the log-probability of the samples for each class in\n",
            " |          the model. The columns correspond to the classes in sorted\n",
            " |          order, as they appear in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  predict_proba(self, X)\n",
            " |      Return probability estimates for the test vector X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : array-like of shape (n_samples, n_classes)\n",
            " |          Returns the probability of the samples for each class in\n",
            " |          the model. The columns correspond to the classes in sorted\n",
            " |          order, as they appear in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for `X`.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(CategoricalNB())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "431faebe",
      "metadata": {
        "id": "431faebe"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "NBCategorical.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}